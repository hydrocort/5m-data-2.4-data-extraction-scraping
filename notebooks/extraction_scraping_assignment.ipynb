{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7748f1c6",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Question: The scraping of `https://www.scrapethissite.com/pages/forms/` in the last section assumes a hardcoded (fixed) no of pages. Can you improve the code by removing the hardcoded no of pages and instead use the `»` button to determine if there are more pages to scrape? Hint: Use a `while` loop.\n",
    "\n",
    "```python\n",
    "def parse_and_extract_rows(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extract table rows from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167a9cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1, found 25 teams\n",
      "Scraped page 2, found 25 teams\n",
      "Scraped page 3, found 25 teams\n",
      "Scraped page 4, found 25 teams\n",
      "Scraped page 5, found 25 teams\n",
      "Scraped page 6, found 25 teams\n",
      "Scraped page 7, found 25 teams\n",
      "Scraped page 8, found 25 teams\n",
      "Scraped page 9, found 25 teams\n",
      "Scraped page 10, found 25 teams\n",
      "Scraped page 11, found 25 teams\n",
      "Scraped page 12, found 25 teams\n",
      "Scraped page 13, found 25 teams\n",
      "Scraped page 14, found 25 teams\n",
      "Scraped page 15, found 25 teams\n",
      "Scraped page 16, found 25 teams\n",
      "Scraped page 17, found 25 teams\n",
      "Scraped page 18, found 25 teams\n",
      "Scraped page 19, found 25 teams\n",
      "Scraped page 20, found 25 teams\n",
      "Scraped page 21, found 25 teams\n",
      "Scraped page 22, found 25 teams\n",
      "Scraped page 23, found 25 teams\n",
      "Scraping complete! Total pages: 24, Total teams: 582\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def parse_and_extract_rows(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extract table rows from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict\n",
    "\n",
    "def scrape_all_pages():\n",
    "    \"\"\"\n",
    "    Scrape all pages from the hockey teams website using pagination detection.\n",
    "    Uses the presence of the 'Next' button (») to determine if there are more pages.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "    rows = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        # Construct URL for current page\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}?page_num={page}\"\n",
    "        \n",
    "        # Make request\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        \n",
    "        # Extract data from current page using the provided function\n",
    "        for row_dict in parse_and_extract_rows(soup):\n",
    "            rows.append(row_dict)\n",
    "        \n",
    "        # Check if there's a \"Next\" button (») to determine if more pages exist\n",
    "        # The Next button has aria-label=\"Next\" and contains the » symbol\n",
    "        next_button = soup.find(\"a\", {\"aria-label\": \"Next\"})\n",
    "        \n",
    "        if not next_button:\n",
    "            # No more pages, break the loop\n",
    "            break\n",
    "        \n",
    "        print(f\"Scraped page {page}, found {len([row for row in parse_and_extract_rows(soup)])} teams\")\n",
    "        \n",
    "        # Move to next page\n",
    "        page += 1\n",
    "        \n",
    "        # Be respectful to the server - pause between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Scraping complete! Total pages: {page}, Total teams: {len(rows)}\")\n",
    "    return rows\n",
    "\n",
    "# Usage\n",
    "all_team_data = scrape_all_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27d9a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Team Name': 'Boston Bruins',\n",
       " 'Year': '1990',\n",
       " 'Wins': '44',\n",
       " 'Losses': '24',\n",
       " 'OT Losses': '',\n",
       " 'Win %': '0.55',\n",
       " 'Goals For (GF)': '299',\n",
       " 'Goals Against (GA)': '264',\n",
       " '+ / -': '35'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_team_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1677d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Team Name': 'Winnipeg Jets',\n",
       " 'Year': '2011',\n",
       " 'Wins': '37',\n",
       " 'Losses': '35',\n",
       " 'OT Losses': '10',\n",
       " 'Win %': '0.451',\n",
       " 'Goals For (GF)': '225',\n",
       " 'Goals Against (GA)': '246',\n",
       " '+ / -': '-21'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_team_data[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
